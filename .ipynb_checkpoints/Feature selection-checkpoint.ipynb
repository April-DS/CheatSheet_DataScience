{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52efb1c9-dcb9-4567-9e8b-fee6e285b32c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Feature Selection with sklearn and Pandas\n",
    "[link](https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dbb3743-3e51-4773-9d56-a056a8fee0af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "[Loading libraries and data](#Loading-libraries-and-data)\n",
    "\n",
    "[Filter Method](#Filter-Method:)\n",
    "\n",
    "[Wrapper Method](#Wrapper-Method:)\n",
    "\n",
    "[Embedded Method](#Embedded-Method:)\n",
    "\n",
    "[PCA](#PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5532d35a-6423-4653-b76c-4e07cac63fdd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "About the dataset:\n",
    "We will be using the built-in Boston dataset which can be loaded through sklearn. We will be selecting features using the above listed methods for the regression problem of predicting the “MEDV” column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "975e7234-8f83-4af0-9aa5-5354d2a6c116",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Loading libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b5e6be8-bc79-4b75-aeaa-93aefd12ff55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4cacd52-aa27-4b3e-b090-c43371036a95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Loading the dataset\n",
    "x = load_boston()\n",
    "df = pd.DataFrame(x.data, columns = x.feature_names)\n",
    "df[\"MEDV\"] = x.target\n",
    "X = df.drop(\"MEDV\",1)   #Feature Matrix\n",
    "y = df[\"MEDV\"]          #Target Variable\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b1ba2c4-d025-4c46-b0d0-deb237131ffe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Filter Method:\n",
    "\n",
    "The filtering here is done using correlation matrix.\n",
    "1. Plot the Pearson correlation heatmap\n",
    "2. Select features which has correlation of above 0.5\n",
    "    The correlation coefficient has values between -1 to 1:\n",
    "    - A value closer to 0 implies weaker correlation (exact 0 implying no correlation).\n",
    "    - A value closer to 1 implies stronger positive correlation.\n",
    "    - A value closer to -1 implies stronger negative correlation.\n",
    "    \n",
    "3. Check the correlation of selected features with each other\n",
    "4. Keep that correlated features which correlation with target variable is higher.\n",
    "5. Use features for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55887934-4cd4-48ac-b2e7-da5ba3e03437",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Using Pearson Correlation\n",
    "plt.figure(figsize=(12,10))\n",
    "cor = df.corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aadf5700-fd8d-41fb-a9fa-6754d716ed91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Correlation with output variable\n",
    "cor_target = abs(cor[\"MEDV\"])\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target>0.5]\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84cfeb32-e9b2-4c24-b82c-8db8ce0e580b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the correlation of selected features with each other\n",
    "print(df[[\"LSTAT\",\"PTRATIO\"]].corr())\n",
    "print(df[[\"RM\",\"LSTAT\"]].corr())\n",
    "# it is seen that the variables RM and LSTAT are highly correlated with each other (-0.613808). \n",
    "# Hence we would keep only one variable and drop the other. \n",
    "# We will keep LSTAT since its correlation with MEDV is higher than that of RM.\n",
    "# After dropping RM, we are left with two feature, LSTAT and PTRATIO. \n",
    "# These are the final features given by Pearson correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74c7508e-fac7-430e-9f20-ec7ecb9bcdae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Wrapper Method:\n",
    "\n",
    "Feed the features to the selected Machine Learning algorithm and based on the model performance you add/remove the features.\n",
    "\n",
    "This is an iterative and **computationally expensive** process but it is **more accurate** than the filter method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d613925-9155-40e6-b8d8-013452c96aad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### i. Backward Elimination\n",
    "1. Feed all the possible features to the model at first.\n",
    "2. Check the performance of the model.\n",
    "3. Iteratively remove the worst performing features one by one till the overall performance of the model comes in acceptable range.\n",
    "\n",
    "The performance metric used here to evaluate feature performance is pvalue. If the pvalue is **above 0.05** then we remove the feature, else we keep it.\n",
    "\n",
    "Here we are using OLS model which stands for “Ordinary Least Squares”. This model is used for performing linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b293048c-f280-40e8-944d-99d02e9a46b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Adding constant column of ones, mandatory for sm.OLS model\n",
    "X_1 = sm.add_constant(X)\n",
    "#Fitting sm.OLS model\n",
    "model = sm.OLS(y,X_1).fit()\n",
    "model.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e7f0c1f-0250-41e3-9f39-75f8a5284378",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# As we can see that the variable ‘AGE’ has highest pvalue of 0.9582293 which is greater than 0.05. \n",
    "# Hence we will remove this feature and build the model once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9181d1f7-d05d-4f10-b3eb-6e55850ca052",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Backward Elimination\n",
    "# This is an iterative process and can be performed at once with the help of loop.\n",
    "cols = list(X.columns)\n",
    "pmax = 1\n",
    "while (len(cols)>0):\n",
    "    p= []\n",
    "    X_1 = X[cols]\n",
    "    X_1 = sm.add_constant(X_1)\n",
    "    model = sm.OLS(y,X_1).fit()\n",
    "    p = pd.Series(model.pvalues.values[1:],index = cols)      \n",
    "    pmax = max(p)\n",
    "    feature_with_p_max = p.idxmax()\n",
    "    if(pmax>0.05):\n",
    "        cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "selected_features_BE = cols\n",
    "print('Final set of variables:',selected_features_BE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0f4eb46-de85-4ed5-8d73-ac00a533fc2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "ii. RFE (Recursive Feature Elimination)\n",
    "Works by recursively removing attributes and building a model on those attributes that remain.\n",
    "\n",
    "Uses **accuracy metric** to rank the feature according to their importance.\n",
    "1. Takes the model to be used and the number of required features as input.\n",
    "2. Gives the ranking of all the variables, 1 being most important.\n",
    "\n",
    "/here 7 (number of required features to remain) is rundom number. Better way is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bc58de2-e714-4df4-be34-50139b55a5ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "#Initializing RFE model\n",
    "rfe = RFE(model, 7)\n",
    "#Transforming data using RFE\n",
    "X_rfe = rfe.fit_transform(X,y)  \n",
    "#Fitting the data to model\n",
    "model.fit(X_rfe,y)\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41f3b72f-ba58-4076-a02c-fd4dcc011242",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here we took LinearRegression model with 7 features and RFE gave feature ranking as above, but the selection of **number ‘7’ was random**.\n",
    "\n",
    "Now we need to find the *optimum* number of features.\n",
    "\n",
    "We do that by using loop starting with 1 feature and going up to 13. We then take the one for which the accuracy is highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4931bd2a-2a4f-412e-9927-164d3931955f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# № of features from 1 to original number of columns\n",
    "nof_list=np.arange(1,13)            \n",
    "high_score=0\n",
    "#Variable to store the optimum features\n",
    "nof=0           \n",
    "score_list =[]\n",
    "for n in range(len(nof_list)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(model,nof_list[n])\n",
    "    X_train_rfe = rfe.fit_transform(X_train,y_train)\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    model.fit(X_train_rfe,y_train)\n",
    "    score = model.score(X_test_rfe,y_test)\n",
    "    score_list.append(score)\n",
    "    if(score>high_score):\n",
    "        high_score = score\n",
    "        nof = nof_list[n]\n",
    "print(\"Optimum number of features: %d\" %nof)\n",
    "print(\"Score (R2) with %d features: %f\" % (nof, high_score))\n",
    "\n",
    "# the optimum number of features is 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11513ef8-1452-4288-ae5e-5b935986e1a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We now feed 10 as number of features to RFE and get the final set of features given by RFE method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bedb0ef9-b58b-4061-8f2d-3f30e73dd7e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols = list(X.columns)\n",
    "model = LinearRegression()\n",
    "# Initializing RFE model\n",
    "rfe = RFE(model, 10)             \n",
    "# Transforming data using RFE\n",
    "X_rfe = rfe.fit_transform(X,y)  \n",
    "# Fitting the data to model\n",
    "model.fit(X_rfe,y)              \n",
    "temp = pd.Series(rfe.support_,index = cols)\n",
    "selected_features_rfe = temp[temp==True].index\n",
    "print(selected_features_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b01b0a75-0b16-4cc0-8821-65b575e0db82",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Embedded Method:\n",
    "\n",
    "Regularization methods are the most commonly used embedded methods which penalize a feature given a coefficient threshold.\n",
    "\n",
    "If the feature is irrelevant, lasso penalizes it’s coefficient and make it 0. Hence the features with coefficient = 0 are removed and the rest are taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "315f2f14-eb0e-442f-91c4-3168a1b97055",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reg = LassoCV()\n",
    "reg.fit(X, y)\n",
    "print(\"Best alpha (penalizer) using built-in LassoCV: %f\" % reg.alpha_)\n",
    "print(\"Best score (R2) using built-in LassoCV: %f\" %reg.score(X,y))\n",
    "\n",
    "coef = pd.Series(reg.coef_, index = X.columns)\n",
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \n",
    "      \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9ac8977-5719-4d99-a85a-9604976f071c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "imp_coef = coef.sort_values()\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Feature importance using Lasso Model\")\n",
    "plt.xlabel('R2 score')\n",
    "plt.ylabel('Features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "517db23b-7f81-4bc4-90ff-173bd3b43688",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Conclusion:\n",
    "\n",
    "Following points will help you make the decision of which method to choose in what situation:\n",
    "1. Filter method is less accurate. It is great while doing EDA, it can also be used for checking multi co-linearity in data.\n",
    "2. Wrapper and Embedded methods give more accurate results but as they are computationally expensive, these method are suited when you have lesser features (~20)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cbe9dc4-4ec0-409a-84b2-99c5e88dbb92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dae4a5d-4b79-439e-a2ce-d10583365d8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "ss = StandardScaler()\n",
    "# Scale X_train.\n",
    "X_train = ss.fit_transform(X_train)\n",
    "# Scale X_test.\n",
    "X_test = ss.transform(X_test)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "# Instantiate linear regression model.\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Transform Z_train and Z_test.\n",
    "Z_train = pca.transform(X_train)\n",
    "# Don't forget to transform the test data!\n",
    "Z_test = pca.transform(X_test)\n",
    "\n",
    "# Fit on Z_train.\n",
    "lm.fit(Z_train, y_train)\n",
    "\n",
    "# Score on training and testing sets.\n",
    "print(f'Training Score: {round(lm.score(Z_train, y_train),4)}')\n",
    "print(f'Testing Score: {round(lm.score(Z_test, y_test),4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b93007ad-ccb2-4b49-a54b-16eb23dd8206",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pull the explained variance attribute.\n",
    "var_exp = pca.explained_variance_ratio_\n",
    "print(f'Explained variance (first 20 components): {np.round(var_exp[:20],3)}')\n",
    "\n",
    "print('')\n",
    "\n",
    "# Generate the cumulative explained variance.\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(f'Cumulative explained variance (first 20 components): {np.round(cum_var_exp[:20],3)}')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Feature selection-checkpoint",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
