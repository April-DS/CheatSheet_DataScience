{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Supervised workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content:\n",
    "\n",
    "- [Model selection](#Model-selection)\n",
    "- [Importing libraries](#Importing-libraries)\n",
    "- [Data Cleaning](#Data-Cleaning)\n",
    "- [Data preprocessing](#Data-preprocessing)\n",
    "- [Text preprocessing](#Text-preprocessing)\n",
    "- [Resampling](#Resampling)\n",
    "- [Models Regressions](#Models-Regressions)\n",
    "\t- [Baseline Model](#Baseline-Model)\n",
    "\t- [Linear Regression](#Linear-Regression)\n",
    "\t- [Lasso](#Lasso)\n",
    "\t- [Ridge Regression](#Ridge-Regression)\n",
    "\t- [ElasticNet Regression](#ElasticNet-Regression)\n",
    "\t- [Metrics regression](#Metrics-regression)\n",
    "- [Models Classifications](#Models-Classifications)\n",
    "\t- [Baseline model](#Baseline-model)\n",
    "\t- [Classifier](#Classifier)\n",
    "\t- [Classifier for text](#Classifier-for-text)\n",
    "\t- [Classifiers evaluation function](#Classifiers-evaluation-function)\n",
    "\t- [Feature Importance](#Feature-Importance)\n",
    "\t- [Metrics Classifiers](#Metrics-Classifiers)\n",
    "- [Pipeline and GridSearch](#Pipeline-and-GridSearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_static/ml_map.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# preprocessing data\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "# preprocessing text\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing import preprocessing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# regressors\n",
    "from sklearn.linear_model import LinearRegressionRidge, RidgeCV\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "# metrics\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score\n",
    "\n",
    "# classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier \n",
    "from sklearn.ensemble import VotingClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "# metrics\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score,roc_auc_score\n",
    "\n",
    "# other\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# resampling\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputation of medians\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures()\n",
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding categorical variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_feature = df.select_dtypes()\n",
    "cat_encoder = OneHotEncoder()\n",
    "cat_encoder.fit_transform(df[categorical_feature])\n",
    "\n",
    "# Get dummies\n",
    "\n",
    "df = pd.get_dummies(data=df,columns=['column1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X - features variables and y - target variable for train and test dataset\n",
    "features = [col for col in df_train.columns if col !='target']\n",
    "X = df_train[features]\n",
    "y = df_train['target']\n",
    "X_test = df_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing train and test features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train_scaled = ss.fit_transform(X) # for train used fit_transform\n",
    "X_test_scaled = ss.transform(X_test) # for test transform only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting scaled train dataset for further verification of model\n",
    "from sklearn.preprocessing import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_scaled,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for t ext preprocessing\n",
    "def text_preprocessing(df,columns_list,is_lem=True,is_stem=True):\n",
    "    '''\n",
    "    Lemmatize, Stemming list of words and concatenates in one string\n",
    "    \n",
    "    Takes:\n",
    "        df - DataFrame\n",
    "        columns_list - (list if str) - list with column' names with list of words\n",
    "        is_lem=True - (bool) - activate WordNetLemmatizer\n",
    "        is_stem=True - (bool) - activate PorterStemmer\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with concatenated list of words\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    p_stemmer = PorterStemmer()\n",
    "    \n",
    "    \n",
    "    for column in columns_list:\n",
    "        # removing tegs\n",
    "        df[column] = df[column].map(lambda x: BeautifulSoup(text, \"lxml\").text)\n",
    "        # removing non-letters\n",
    "        df[column] = df[column].map(lambda x:re.sub(\"[^a-zA-Z]\", \" \", x))\n",
    "        # Instantiating Tokenizer and setting a pattern to only words\n",
    "        # applying Tokenizer to texts\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        df[column] = df[column].map(lambda x: tokenizer.tokenize(x.lower()))\n",
    "        if is_lem:\n",
    "            df[column] = df[column].apply(lambda row: [lemmatizer.lemmatize(text)\n",
    "                                     for text in row])\n",
    "        if is_stem:\n",
    "            df[column] = df[column].apply(lambda row: [p_stemmer.stem(text)\n",
    "                                     for text in row])\n",
    "        df[column] = df[column].apply(lambda row: ' '.join(word for word in row))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating list of words in strings row-wise, \n",
    "# disabling Lemmatizer and PorterStemmer for better readability of words, \n",
    "# number of words in posts is not high so Lemmatizer and PorterStemmer are not necessary\n",
    "df = text_preprocessing(df,['text'],is_lem=False,is_stem=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping binary target\n",
    "df['target'] = df['target'].map({'yes':1,'no':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom stop words\n",
    "all_stop_words = set(list(stopwords.words('english')) + list(preprocessing.STOPWORDS) + \n",
    "                    ['https','www'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing two vectorizers\n",
    "# maximum number of words is 5000, used custom stopwords\n",
    "cvec = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = all_stop_words,\n",
    "                             max_features = 5000)\n",
    "# CountVectorizer\n",
    "train_features_cvec = cvec.fit_transform(X_train)\n",
    "print(train_features_cvec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing words for train dataset: fit the model and learn train vocabulary \n",
    "# and transforming strings info feature vectors\n",
    "tvec = TfidfVectorizer(stop_words = all_stop_words,\n",
    "                             max_features = 5000)\n",
    "# TfidfVectorizer\n",
    "train_features_tvec = tvec.fit_transform(X_train)\n",
    "print(train_features_tvec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function for resamling\n",
    "def resampling_dataset(how,X_train,y_train,on= True):\n",
    "    \"\"\"\n",
    "    Resamples imbalanced dataset\n",
    "    \n",
    "    Takes:\n",
    "    how (str) : 'under', 'over', 'smote'\n",
    "    x - df - with features\n",
    "    y - series - with target\n",
    "    on - on/of function\n",
    "    Returns:\n",
    "    X,y\n",
    "    \"\"\"\n",
    "    if on:\n",
    "        if how == 'under':\n",
    "            X = pd.concat([X_train, y_train],axis=1)\n",
    "            true_virus = X[X['wnvpresent']==1]\n",
    "            false_virus = X[X['wnvpresent']==0]\n",
    "            false_virus_downsampled = resample(false_virus,\n",
    "                                    replace = False, # sample without replacement\n",
    "                                    n_samples = len(true_virus), # match minority n\n",
    "                                    random_state = 42) # reproducible results\n",
    "\n",
    "            # combine minority and downsampled majority\n",
    "            downsampled = pd.concat([false_virus_downsampled, true_virus])\n",
    "            features = [col for col in downsampled if col !='wnvpresent']\n",
    "            X_unders = downsampled[features]\n",
    "            y_unders = downsampled['wnvpresent']\n",
    "            return X_unders, y_unders\n",
    "        elif how == 'over':\n",
    "            sm = SMOTE(sampling_strategy='minority',random_state=42)\n",
    "            X_train_sm, y_train_sm = sm.fit_sample(X_train, y_train)\n",
    "            oversempling_smote = pd.concat([X_train_sm, y_train_sm],axis=1)\n",
    "            return X_train_sm, y_train_sm\n",
    "        else:\n",
    "            smt = SMOTETomek(sampling_strategy='all')\n",
    "            X_smt, y_smt = smt.fit_sample(X_train, y_train)\n",
    "            return X_smt, y_smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampling_dataset('over',X,y,on=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model for Regression - mean\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "yhat = [np.mean(y) for i in range(len(y))]\n",
    "test_rmse = np.sqrt(mean_squared_error(y_true=y, y_pred=yhat))\n",
    "test_r2 = r2_score(y_true=y, y_pred=yhat)\n",
    "print('--- Baseline model scores ---')\n",
    "print('Root mean squared error RMSE:', test_rmse)\n",
    "print('R2:', test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_sc,y_train)\n",
    "print(\"Coefficients\", lr.coef_)\n",
    "predictions  =  lr.predict(X_test_sc)\n",
    "print('Score:',lr.score(X_test_sc,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "\n",
    "lasso_alpha = np.arange(0.001,0.15,0.0025)\n",
    "lasso_model = LassoCV(alphas=lasso_alpha,cv=5)\n",
    "lasso_model.fit(X,y)\n",
    "opt_alpha = lasso_model.alpha_\n",
    "lasso_optimal_model = Lasso(alpha=opt_alpha)\n",
    "lasso_optimal_model.fit(X,y)\n",
    "predictions = lasso_optimal_model.predict(X_test)\n",
    "#  all 0 is usless columns\n",
    "lasso_optimal_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 10\n",
    "ridge_model = Ridge(alpha = 10)\n",
    "r_alpha = np.logspace(0,5,200)\n",
    "ridge_model = RidgeCV(alphas = r_alpha,store_cv_values=True)\n",
    "ridge_model.fit(X,y)\n",
    "ridge_optimal_alpha = ridge_model.alpha_\n",
    "ridge_optimal = Ridge(alpha=ridge_optimal_alpha)\n",
    "print(cross_val_score(ridge_optimal,X,y).mean())\n",
    "ridge_optimal.fit(X,y)\n",
    "predictions = ridge_optimal.predict(X_test)\n",
    "ridge_optimal.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "\n",
    "enet_alpha = np.arange(0.1,1,0.05)\n",
    "ent_lratio = 0.5 # 50% of Lasso, 50% of Ridge\n",
    "enet_model = ElasticNetCV(alphas=enet_alpha,l1_ratio=ent_lratio,cv=5)\n",
    "enet_model.fit(X_overfit,y)\n",
    "enet_optimal_alpha = enet_model.alpha_\n",
    "lasso_optimal_model = Lasso(alpha=opt_alpha)\n",
    "lasso_optimal_model.fit(X,y)\n",
    "predictions = lasso_optimal_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for metrics\n",
    "def metrics_function(y,predictions,num_features=len(features)):\n",
    "    # Mean Absolute Error\n",
    "    mae = mean_absolute_error(y,predictions)\n",
    "    # Residual Sum of Squares\n",
    "    rss = ((y-predictions)**2).sum()\n",
    "    # Mean Squared Error\n",
    "    mse = mean_squared_error(y,predictions)\n",
    "    # Root Mean Squared Error\n",
    "    rmse = mse**0.5\n",
    "    # Coefficient of Determination\n",
    "    r = r2_score(y,predictions)\n",
    "    # Adjusted R2\n",
    "    def r2_adj(y,predictions,num_features):\n",
    "        r_adj = 1 - (1-r)*(len(y)-1)/(len(y)-num_features-1)\n",
    "        return r_adj\n",
    "    r_adj = r2_adj(y,predictions,num_features)\n",
    "    print(f'Mean Absolute Error: {mae}')\n",
    "    print(f'Residual Sum of Squares: {rss}')\n",
    "    print(f'Mean Squared Error: {mse}')\n",
    "    print(f'Root Mean Squared Error: {rmse}')\n",
    "    print(f'Coefficient of Determination R2: {r}')\n",
    "    print(f'Adjusted R2: {r_adj}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "cross_scores = cross_val_score(lr,X_train,y_train)\n",
    "lr.fit(X_train,y_train)\n",
    "train_score = lr.score(X_train, y_train)\n",
    "test_score = lr.score(X_test, y_test)\n",
    "predictions = lr.predict(X_test)\n",
    "predictions = lr.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer with MultinomialNB\n",
    "tvec = TfidfVectorizer(stop_words = all_stop_words)\n",
    "nb = MultinomialNB()\n",
    "X_train_tv = tvec.fit_transform(X_train)\n",
    "X_test_tv = tvec.transform(X_test)\n",
    "cross_scores = cross_val_score(nb,X_train_tv.todense(),y_train)\n",
    "print(f'Cross_val_scores: {[round(i,3) for i in cross_scores]}')\n",
    "nb.fit(X_train_tv.todense(),y_train)\n",
    "train_score = nb.score(X_train_tv.todense(), y_train)\n",
    "print(f'Train score: {round(train_score,3)}')\n",
    "test_score = nb.score(X_test_tv.todense(), y_test)\n",
    "print(f'Test score: {round(test_score,3)}')\n",
    "predictions = nb.predict(X_test_tv)\n",
    "model_metrics(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer with KNN\n",
    "tvec_knn = TfidfVectorizer(best_param,stop_words = all_stop_words)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=10,jobs=-1)\n",
    "X_train_tv_knn = tvec_knn.fit_transform(X_train)\n",
    "X_test_tv_knn = tvec_knn.transform(X_test)\n",
    "cross_scores = cross_val_score(knn_model,X_train_tv_knn,y_train)\n",
    "print(f'Cross_val_scores: {[round(i,3) for i in cross_scores]}')\n",
    "knn_model.fit(X_train_tv_knn,y_train)\n",
    "train_score = knn_model.score(X_train_tv_knn, y_train)\n",
    "print(f'Train score: {round(train_score,3)}')\n",
    "test_score = knn_model.score(X_test_tv_knn, y_test)\n",
    "print(f'Test score: {round(test_score,3)}')\n",
    "predictions = knn_model.predict(X_test_tv_knn)\n",
    "model_metrics(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with list of true values and predicted probabilities based on our model\n",
    "pred_proba = [i[1] for i in lr.predict_proba(X_test_tv)]\n",
    "pred_df = pd.DataFrame({'true_values': y_test,\n",
    "                        'pred_probs':pred_proba})\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function for simple models evaluation\n",
    "def sample_evaluation(X,y,models,names_samples,X_test=X_test,y_test=y_test,on= True):\n",
    "    if on:\n",
    "        for i in range(len(X)):\n",
    "            ss = StandardScaler()\n",
    "            X_train_scaled = ss.fit_transform(X[i])\n",
    "            X_test_scaled = ss.transform(X_test)\n",
    "            for model in models:\n",
    "                cv_scores = cross_val_score(model,X_train_scaled,y[i])\n",
    "                model.fit(X_train_scaled,y[i])\n",
    "                train_score = model.score(X_train_scaled,y[i])\n",
    "                test_score = model.score(X_test_scaled,y_test)\n",
    "                print(names_samples[i])\n",
    "                print(str(model).split('(')[0])\n",
    "                print('CV',cv_scores)\n",
    "                print('train',train_score)\n",
    "                print('test',test_score)\n",
    "                # set of predicted labels match the corresponding set of true labels\n",
    "                y_predicted = model.predict(X_test_scaled)\n",
    "                try:\n",
    "                    pred_proba = [i[1] for i in model.predict_proba(X_test_scaled)]\n",
    "                    pred_df = pd.DataFrame({'true_values': y_test,\n",
    "                                'pred_probs':pred_proba})\n",
    "                    print(f'ROC score {round(roc_auc_score(pred_df[\"true_values\"], pred_df[\"pred_probs\"]),3)}')\n",
    "                except:\n",
    "                    pass\n",
    "                accuracy = accuracy_score(y_test, y_predicted)\n",
    "                # ratio tp / (tp + fp)\n",
    "                precision = precision_score(y_test, y_predicted)             \n",
    "                # ratio tp / (tp + fn)\n",
    "                recall = recall_score(y_test, y_predicted)\n",
    "                # weighted average of the precision and recall\n",
    "                # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "                f1 = f1_score(y_test, y_predicted)\n",
    "                tn, fp, fn, tp = confusion_matrix(y_test, y_predicted).ravel()\n",
    "                print(\"True Negatives: %s\" % tn)\n",
    "                print(\"False Positives: %s\" % fp)\n",
    "                print(\"False Negatives: %s\" % fn)\n",
    "                print(\"True Positives: %s\" % tp)\n",
    "                print(f'accuracy {round(accuracy,3)}, precision {round(precision,3)},recall {round(recall,3)}, f1 {round(f1,3)}')\n",
    "                print('==================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change on to True to run a function\n",
    "lr = LogisticRegression()\n",
    "knn = KNeighborsClassifier()\n",
    "svm = SVC()\n",
    "dtree = DecisionTreeClassifier()\n",
    "randtree = RandomForestClassifier()\n",
    "sample_evaluation([X_unders, X_train_sm, X_smt],[y_unders, y_train_sm,y_smt],\n",
    "                  models=[lr,knn,svm,dtree,randtree,],\n",
    "                  names_samples=['undersampling','oversampling','over-undersampling'],\n",
    "                 on= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function for models evaluation with gridsearch parameters\n",
    "def sample_evaluation_grid(X,y,models,params,names_samples,X_test=X_test,y_test=y_test,on=True):\n",
    "    \"\"\"\n",
    "    Use different resample and different ML models to evaluate perfomance with gridsearch\n",
    "    Takes:\n",
    "    X - list of resampled X\n",
    "    y - list of resampled y\n",
    "    params - list of hyperparameters for gridsearch\n",
    "    name_samples - list of str - name of resampling technique\n",
    "    \n",
    "    Prints accoracy score on train and test data\n",
    "    \"\"\"\n",
    "    if on:\n",
    "        for i in range(len(X)):\n",
    "            X_train = X[i]\n",
    "            y_train = y[i]\n",
    "            ss = StandardScaler()\n",
    "            X_train_scaled = ss.fit_transform(X_train)\n",
    "            X_test_scaled = ss.transform(X_test)\n",
    "            for j, model in enumerate(models):\n",
    "                grid = GridSearchCV(model,param_grid=params[j])\n",
    "                cv_scores = cross_val_score(grid,X_train_scaled,y_train)\n",
    "                grid.fit(X_train_scaled,y_train)\n",
    "                best_model = grid.best_estimator_\n",
    "                best_model.fit(X_train_scaled,y_train)\n",
    "                train_score = best_model.score(X_train_scaled,y_train)\n",
    "                test_score = best_model.score(X_test_scaled,y_test)\n",
    "                print(names_samples[i])\n",
    "                print(str(model).split('(')[0])\n",
    "                print(grid.best_params_)\n",
    "                y_predicted = best_model.predict(X_test_scaled)\n",
    "                try:\n",
    "                    pred_proba = [i[1] for i in best_model.predict_proba(X_test_scaled)]\n",
    "                    pred_df = pd.DataFrame({'true_values': y_test,\n",
    "                                'pred_probs':pred_proba})\n",
    "                    print(f'ROC score {round(roc_auc_score(pred_df[\"true_values\"], pred_df[\"pred_probs\"]),3)}')\n",
    "                except:\n",
    "                    pass\n",
    "                print('CV',cv_scores)\n",
    "                print('train',train_score)\n",
    "                print('test',test_score)\n",
    "                # set of predicted labels match the corresponding set of true labels\n",
    "                accuracy = accuracy_score(y_test, y_predicted)\n",
    "                # ratio tp / (tp + fp)\n",
    "                precision = precision_score(y_test, y_predicted)             \n",
    "                # ratio tp / (tp + fn)\n",
    "                recall = recall_score(y_test, y_predicted)\n",
    "                # weighted average of the precision and recall\n",
    "                # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "                f1 = f1_score(y_test, y_predicted)\n",
    "                tn, fp, fn, tp = confusion_matrix(y_test, y_predicted).ravel()\n",
    "                print(\"True Negatives: %s\" % tn)\n",
    "                print(\"False Positives: %s\" % fp)\n",
    "                print(\"False Negatives: %s\" % fn)\n",
    "                print(\"True Positives: %s\" % tp)\n",
    "                print(f'accuracy {round(accuracy,3)}, precision {round(precision,3)},recall {round(recall,3)}, f1 {round(f1,3)}')\n",
    "                print('==================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(n_jobs=-1)\n",
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "svm = SVC()\n",
    "dtree = DecisionTreeClassifier()\n",
    "randtree = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# Change on to True to run a function\n",
    "sample_evaluation_grid([X],[y],\n",
    "models=[lr,knn,svm,dtree,randtree,],names_samples=['oversampling'],\n",
    "params = [{'penalty':['l2','none'],'tol':[0.0001,0.001],'max_iter':[500,700]},\n",
    "         {'n_neighbors':[3,5,7], 'weights':['uniform','distance']},\n",
    "         {'degree':[1,3,6], 'C':[0.1,0.3,1]},\n",
    "        {'max_depth':[None,2,4],'min_samples_leaf':[1,2,3]},\n",
    "        {'n_estimators':[80,100,120],'max_depth':[None,2,4],'min_samples_leaf':[1,2,3]}],\n",
    "                       on= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradb = GradientBoostingClassifier()\n",
    "bag = BaggingClassifier()\n",
    "ada = AdaBoostClassifier(RandomForestClassifier(max_depth=None,min_samples_leaf=1))\n",
    "sample_evaluation([X],[y],models=[gradb],names_samples=['oversampling'],on=True)\n",
    "sample_evaluation_grid([X],[y],models=[bag,ada],names_samples=['oversampling'],params = [\n",
    "             {'base_estimator':[KNeighborsClassifier(n_jobs=-1,n_neighbors=7,weights='distance'),\n",
    "                                RandomForestClassifier(max_depth=None,min_samples_leaf=1)]},\n",
    "             {'base_estimator':[DecisionTreeClassifier(),\n",
    "                                RandomForestClassifier(max_depth=None,min_samples_leaf=1)]}\n",
    "             ],on= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Classifier\n",
    "# Evaluation of ensambles of models\n",
    "X,y = resampling_dataset('over',X,y,on= True)\n",
    "\n",
    "# Creation a voting classifier with best models\n",
    "vc = VotingClassifier(estimators=[('knn',KNeighborsClassifier(weights='distance')),\n",
    "                                  ('ada',AdaBoostClassifier(base_estimator=DecisionTreeClassifier())),\n",
    "                                  ('bagg',BaggingClassifier(base_estimator=RandomForestClassifier(min_samples_leaf=2))),\n",
    "                                  ('rfor',RandomForestClassifier()),('grad',GradientBoostingClassifier())\n",
    "                                 ],voting='soft')\n",
    "# Using gridsearch for better evaluation\n",
    "on= True\n",
    "if on:\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    ss = StandardScaler()\n",
    "    X_train_scaled = ss.fit_transform(X_train)\n",
    "    X_test_scaled = ss.transform(X_test)\n",
    "    \n",
    "    vc.fit(X_train_scaled,y_train)\n",
    "    print(vc.score(X_train_scaled,y_train))\n",
    "    print(vc.score(X_test_scaled,y_test))\n",
    "    # Evaluation\n",
    "    # set of predicted labels match the corresponding set of true labels\n",
    "    y_predicted = vc.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    # ratio tp / (tp + fp)\n",
    "    precision = precision_score(y_test, y_predicted)             \n",
    "    # ratio tp / (tp + fn)\n",
    "    recall = recall_score(y_test, y_predicted)\n",
    "    # weighted average of the precision and recall\n",
    "    # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    f1 = f1_score(y_test, y_predicted)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_predicted).ravel()\n",
    "    print(\"True Negatives: %s\" % tn)\n",
    "    print(\"False Positives: %s\" % fp)\n",
    "    print(\"False Negatives: %s\" % fn)\n",
    "    print(\"True Positives: %s\" % tp)\n",
    "    print(f'accuracy {round(accuracy,3)}, precision {round(precision,3)},recall {round(recall,3)}, f1 {round(f1,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,1)\n",
    "np.ravel(ax)\n",
    "for i,model in enumerate([dtree,randtree]):\n",
    "    feat_importances = pd.Series(model.feature_importances_, index=X_test.columns)\n",
    "    feat_importances.nlargest(10).plot(kind='barh', figsize=(15,15),ax=ax[i])\n",
    "    model_name = str(model).split('(')[0]\n",
    "    ax[i].set_title(f'10 Most important features of {model_name}',fontsize=18)\n",
    "    for tick in ax[i].yaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_metrics(y_test, y_predicted):  \n",
    "    \"\"\"\n",
    "    Calculates accuracy, precision, recall, f1\n",
    "    \n",
    "    Takes:\n",
    "    y_test - pandas Series\n",
    "    y_predicted - pandas Series\n",
    "    \n",
    "    Prints accuracy, precision, recall, f1\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # set of predicted labels match the corresponding set of true labels\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    # ratio tp / (tp + fp)\n",
    "    precision = precision_score(y_test, y_predicted)             \n",
    "    # ratio tp / (tp + fn)\n",
    "    recall = recall_score(y_test, y_predicted)\n",
    "    # weighted average of the precision and recall\n",
    "    # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    f1 = f1_score(y_test, y_predicted)\n",
    "    roc = roc_auc_score(y_test, y_predicted)\n",
    "    print(f'ROC score {round(roc),3}')\n",
    "    print(f'accuracy {round(accuracy,3)}, precision {round(precision,3)},recall {round(recall,3)}, f1 {round(f1,3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "disp = plot_confusion_matrix(vc,X_test_scaled,y_test,normalize='true',\n",
    "                             display_labels=['Yes','No'])\n",
    "disp.ax_.set_title('Confusion Matrix');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating distribution of divided probability\n",
    "def div_prob(pred_proba,y_test=y_test):\n",
    "\n",
    "    pred_df = pd.DataFrame({'true_values': y_test,\n",
    "                            'pred_probs':pred_proba})\n",
    "    plt.figure(figsize = (10,7))\n",
    "    # Create two histograms of observations.\n",
    "    plt.hist(pred_df[pred_df['true_values'] == 0]['pred_probs'],\n",
    "             bins=25,\n",
    "             color='#65a8a7',\n",
    "             alpha = 0.5,\n",
    "             label='WNVirus not present')\n",
    "    plt.hist(pred_df[pred_df['true_values'] == 1]['pred_probs'],\n",
    "             bins=25,\n",
    "             color='#fcba03',\n",
    "             alpha = 0.5,\n",
    "             label='WNVirus present')\n",
    "\n",
    "    # Add vertical line at P(Outcome = 1) = 0.5.\n",
    "    plt.vlines(x=0.5,\n",
    "               ymin = 0,\n",
    "               ymax = 65,\n",
    "               color='r',\n",
    "               linestyle = '--')\n",
    "\n",
    "    # Label axes.\n",
    "    plt.title('Distribution of Probability', fontsize=22)\n",
    "    plt.ylabel('Frequency', fontsize=18)\n",
    "    plt.xlabel('Predicted Probability that Outcome = 1', fontsize=18)\n",
    "    plt.text(y = 40,x = 0.1,s = 'True negative',color='blue')\n",
    "    plt.text(y = 10,x = 0.25,s = 'False negative',color='orange')\n",
    "    plt.text(y = 10,x = 0.55,s = 'False positive',color='blue')\n",
    "    plt.text(y = 40,x = 0.75,s = 'True positive',color='orange')\n",
    "    # Create legend.\n",
    "    plt.legend(fontsize=20,loc='upper center');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "pred_proba = [i[1] for i in vc.predict_proba(X_test_scaled)]\n",
    "div_prob(pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Receiver Operating Characteristic (ROC) Curve\n",
    "def roc_curve(pred_proba,y_test=y_test):\n",
    "    \n",
    "    pred_df = pd.DataFrame({'true_values': y_test,\n",
    "                            'pred_probs':pred_proba})\n",
    "    plt.figure(figsize = (10,7))\n",
    "\n",
    "    # Create threshold values. (Dashed red line in image.)\n",
    "    thresholds = np.linspace(0, 1, 200)\n",
    "\n",
    "    # Define function to calculate sensitivity. (True positive rate.)\n",
    "    def TPR(df, true_col, pred_prob_col, threshold):\n",
    "        true_positive = df[(df[true_col] == 1) & (df[pred_prob_col] >= threshold)].shape[0]\n",
    "        false_negative = df[(df[true_col] == 1) & (df[pred_prob_col] < threshold)].shape[0]\n",
    "        return true_positive / (true_positive + false_negative)\n",
    "\n",
    "\n",
    "    # Define function to calculate 1 - specificity. (False positive rate.)\n",
    "    def FPR(df, true_col, pred_prob_col, threshold):\n",
    "        true_negative = df[(df[true_col] == 0) & (df[pred_prob_col] <= threshold)].shape[0]\n",
    "        false_positive = df[(df[true_col] == 0) & (df[pred_prob_col] > threshold)].shape[0]\n",
    "        return 1 - (true_negative / (true_negative + false_positive))\n",
    "\n",
    "    # Calculate sensitivity & 1-specificity for each threshold between 0 and 1.\n",
    "    tpr_values = [TPR(pred_df, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "    fpr_values = [FPR(pred_df, 'true_values', 'pred_probs', prob) for prob in thresholds]\n",
    "\n",
    "    # Plot ROC curve.\n",
    "    plt.plot(fpr_values, # False Positive Rate on X-axis\n",
    "             tpr_values, # True Positive Rate on Y-axis\n",
    "             label='ROC Curve')\n",
    "\n",
    "    # Plot baseline. (Perfect overlap between the two populations.)\n",
    "    plt.plot(np.linspace(0, 1, 200),\n",
    "             np.linspace(0, 1, 200),\n",
    "             label='baseline',\n",
    "             linestyle='--')\n",
    "\n",
    "    # Label axes.\n",
    "    plt.title(f'ROC Curve with AUC = {round(roc_auc_score(pred_df[\"true_values\"], pred_df[\"pred_probs\"]),3)}', fontsize=22)\n",
    "    plt.ylabel('Recall', fontsize=18)\n",
    "    plt.xlabel('1 - Specificity', fontsize=18)\n",
    "\n",
    "    # Create legend.\n",
    "    plt.legend(fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve(pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline and GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    # try 6 (2Ã—3) combinations of hyperparameters\n",
    "    {'n_estimators': [5, 10], 'max_features': [6, 8, 10]}]\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "# train across 3 folds, that's a total of 6*3=18 rounds of training \n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=3,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
